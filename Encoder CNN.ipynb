{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3542461865.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[5], line 6\u001B[1;36m\u001B[0m\n\u001B[1;33m    from tensorflow.keras.optimizers import A!am\u001B[0m\n\u001B[1;37m                                             ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import A!am\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "# Load the CSV file with image names and classes\n",
    "csv_path = r\"C:\\Users\\a0494\\PycharmProjects\\int20h\\emotion_labels.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df['Label'] = df['Label'].astype(str)\n",
    "# Define image dimensions and classes\n",
    "img_width, img_height, img_channels = 128, 128, 3\n",
    "num_classes = len(df['Label'].unique())\n",
    "\n",
    "# Data augmentation using ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # split the data into training and validation sets\n",
    ")\n",
    "\n",
    "# Create generators for training and validation data\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory='data/crop_sorted_images',\n",
    "    x_col='Filename',\n",
    "    y_col='Label',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory='data/crop_sorted_images',\n",
    "    x_col='Filename',\n",
    "    y_col='Label',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Build the CNN model\n",
    "original_model = Sequential()\n",
    "original_model.add(Conv2D(32, (3, 3), input_shape=(img_width, img_height, img_channels), activation='relu'))\n",
    "original_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "original_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "original_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "original_model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "original_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "original_model.add(Flatten())\n",
    "original_model.add(Dense(128, activation='relu'))\n",
    "original_model.add(Dense(7, activation='relu'))\n",
    "original_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Index of the layer you want to use as an encoder\n",
    "encoder_layer_index = 8  # Assuming 0-based index\n",
    "\n",
    "# Create a new model using the layers up to the encoder layer\n",
    "encoder_model = Model(inputs=original_model.input, outputs=original_model.layers[encoder_layer_index].output)\n",
    "\n",
    "# Compile the encoder model if needed\n",
    "encoder_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Compile the model\n",
    "original_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "original_model.fit(train_generator, epochs=10, validation_data=valid_generator)\n",
    "\n",
    "# Save the trained model\n",
    "encoder_model.save('enc_eth.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.15.0-cp310-cp310-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting tensorflow-intel==2.15.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.15.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached h5py-3.10.0-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached ml_dtypes-0.2.0-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting packaging (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting setuptools (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached setuptools-69.1.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting six>=1.12.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached wrapt-1.14.1-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached grpcio-1.62.0-cp310-cp310-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached wheel-0.42.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached google_auth-2.28.1-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Using cached tensorflow-2.15.0-cp310-cp310-win_amd64.whl (2.1 kB)\n",
      "Using cached tensorflow_intel-2.15.0-cp310-cp310-win_amd64.whl (300.9 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.62.0-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "Using cached h5py-3.10.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Using cached ml_dtypes-0.2.0-cp310-cp310-win_amd64.whl (938 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached setuptools-69.1.1-py3-none-any.whl (819 kB)\n",
      "Using cached tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Using cached wrapt-1.14.1-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Using cached google_auth-2.28.1-py2.py3-none-any.whl (186 kB)\n",
      "Using cached google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Using cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, setuptools, pyasn1, protobuf, packaging, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, opt-einsum, ml-dtypes, h5py, google-pasta, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: libclang\n",
      "    Found existing installation: libclang 16.0.6\n",
      "    Uninstalling libclang-16.0.6:\n",
      "      Successfully uninstalled libclang-16.0.6\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 23.5.26\n",
      "    Uninstalling flatbuffers-23.5.26:\n",
      "      Successfully uninstalled flatbuffers-23.5.26\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.14.1\n",
      "    Uninstalling wrapt-1.14.1:\n",
      "      Successfully uninstalled wrapt-1.14.1\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.42.0\n",
      "    Uninstalling wheel-0.42.0:\n",
      "      Successfully uninstalled wheel-0.42.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.4.0\n",
      "    Uninstalling termcolor-2.4.0:\n",
      "      Successfully uninstalled termcolor-2.4.0\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.31.0\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.31.0:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.31.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.15.0\n",
      "    Uninstalling tensorflow-estimator-2.15.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 69.1.1\n",
      "    Uninstalling setuptools-69.1.1:\n",
      "      Successfully uninstalled setuptools-69.1.1\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.5.1\n",
      "    Uninstalling pyasn1-0.5.1:\n",
      "      Successfully uninstalled pyasn1-0.5.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "  Attempting uninstall: oauthlib\n",
      "    Found existing installation: oauthlib 3.2.2\n",
      "    Uninstalling oauthlib-3.2.2:\n",
      "      Successfully uninstalled oauthlib-3.2.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.5\n",
      "    Uninstalling MarkupSafe-2.1.5:\n",
      "      Successfully uninstalled MarkupSafe-2.1.5\n",
      "  Attempting uninstall: markdown\n",
      "    Found existing installation: Markdown 3.5.2\n",
      "    Uninstalling Markdown-3.5.2:\n",
      "      Successfully uninstalled Markdown-3.5.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.6\n",
      "    Uninstalling idna-3.6:\n",
      "      Successfully uninstalled idna-3.6\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.62.0\n",
      "    Uninstalling grpcio-1.62.0:\n",
      "      Successfully uninstalled grpcio-1.62.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.4\n",
      "    Uninstalling gast-0.5.4:\n",
      "      Successfully uninstalled gast-0.5.4\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.2\n",
      "    Uninstalling charset-normalizer-3.3.2:\n",
      "      Successfully uninstalled charset-normalizer-3.3.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.2.2\n",
      "    Uninstalling certifi-2024.2.2:\n",
      "      Successfully uninstalled certifi-2024.2.2\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.3.3\n",
      "    Uninstalling cachetools-5.3.3:\n",
      "      Successfully uninstalled cachetools-5.3.3\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.1.0\n",
      "    Uninstalling absl-py-2.1.0:\n",
      "      Successfully uninstalled absl-py-2.1.0\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 3.0.1\n",
      "    Uninstalling Werkzeug-3.0.1:\n",
      "      Successfully uninstalled Werkzeug-3.0.1\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.9\n",
      "    Uninstalling rsa-4.9:\n",
      "      Successfully uninstalled rsa-4.9\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyasn1-modules\n",
      "    Found existing installation: pyasn1-modules 0.3.0\n",
      "    Uninstalling pyasn1-modules-0.3.0:\n",
      "      Successfully uninstalled pyasn1-modules-0.3.0\n",
      "  Attempting uninstall: opt-einsum\n",
      "    Found existing installation: opt-einsum 3.3.0\n",
      "    Uninstalling opt-einsum-3.3.0:\n",
      "      Successfully uninstalled opt-einsum-3.3.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.2.0\n",
      "    Uninstalling ml-dtypes-0.2.0:\n",
      "      Successfully uninstalled ml-dtypes-0.2.0\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 3.10.0\n",
      "    Uninstalling h5py-3.10.0:\n",
      "      Successfully uninstalled h5py-3.10.0\n",
      "  Attempting uninstall: google-pasta\n",
      "    Found existing installation: google-pasta 0.2.0\n",
      "    Uninstalling google-pasta-0.2.0:\n",
      "      Successfully uninstalled google-pasta-0.2.0\n",
      "  Attempting uninstall: astunparse\n",
      "    Found existing installation: astunparse 1.6.3\n",
      "    Uninstalling astunparse-1.6.3:\n",
      "      Successfully uninstalled astunparse-1.6.3\n",
      "  Attempting uninstall: requests-oauthlib\n",
      "    Found existing installation: requests-oauthlib 1.3.1\n",
      "    Uninstalling requests-oauthlib-1.3.1:\n",
      "      Successfully uninstalled requests-oauthlib-1.3.1\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.28.1\n",
      "    Uninstalling google-auth-2.28.1:\n",
      "      Successfully uninstalled google-auth-2.28.1\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.2.0\n",
      "    Uninstalling google-auth-oauthlib-1.2.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.15.2\n",
      "    Uninstalling tensorboard-2.15.2:\n",
      "      Successfully uninstalled tensorboard-2.15.2\n",
      "  Attempting uninstall: tensorflow-intel\n",
      "    Found existing installation: tensorflow-intel 2.15.0\n",
      "    Uninstalling tensorflow-intel-2.15.0:\n",
      "      Successfully uninstalled tensorflow-intel-2.15.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.15.0\n",
      "    Uninstalling tensorflow-2.15.0:\n",
      "      Successfully uninstalled tensorflow-2.15.0\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.28.1 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.62.0 h5py-3.10.0 idna-3.6 keras-2.15.0 libclang-16.0.6 markdown-3.5.2 ml-dtypes-0.2.0 numpy-1.26.4 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.2 protobuf-4.25.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 setuptools-69.1.1 six-1.16.0 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-intel-2.15.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 typing-extensions-4.10.0 urllib3-2.2.1 werkzeug-3.0.1 wheel-0.42.0 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albumentations 1.3.1 requires opencv-python-headless>=4.1.1, which is not installed.\n",
      "qudida 0.0.4 requires opencv-python-headless>=4.0.1, which is not installed.\n",
      "httpx 0.13.3 requires idna==2.*, but you have idna 3.6 which is incompatible.\n",
      "numba 0.57.0 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\n",
      "fastapi 0.105.0 requires anyio<4.0.0,>=3.7.1, but you have anyio 3.6.2 which is incompatible.\n",
      "jupyterlab-server 2.25.2 requires jsonschema>=4.18.0, but you have jsonschema 4.17.3 which is incompatible.\n",
      "mediapipe 0.10.10 requires protobuf<4,>=3.11, but you have protobuf 4.25.3 which is incompatible.\n",
      "openai 1.4.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
      "pydantic-extra-types 2.2.0 requires pydantic>=2.0.3, but you have pydantic 1.10.7 which is incompatible.\n",
      "pydantic-settings 2.1.0 requires pydantic>=2.3.0, but you have pydantic 1.10.7 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 640 validated image filenames belonging to 6 classes.\n",
      "Found 160 validated image filenames belonging to 6 classes.\n",
      "WARNING:tensorflow:From C:\\Users\\a0494\\PycharmProjects\\interp\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a0494\\PycharmProjects\\interp\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 10s 0us/step\n",
      "WARNING:tensorflow:From C:\\Users\\a0494\\PycharmProjects\\interp\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\a0494\\PycharmProjects\\interp\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\a0494\\PycharmProjects\\interp\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "20/20 [==============================] - 16s 681ms/step - loss: 1.5268 - accuracy: 0.5891 - val_loss: 6.0139 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 13s 630ms/step - loss: 1.1148 - accuracy: 0.7047 - val_loss: 3.9107 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 13s 672ms/step - loss: 1.0315 - accuracy: 0.7156 - val_loss: 4.5418 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 13s 631ms/step - loss: 1.0148 - accuracy: 0.7172 - val_loss: 3.8378 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 12s 587ms/step - loss: 1.0169 - accuracy: 0.7156 - val_loss: 4.2498 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 12s 594ms/step - loss: 0.9981 - accuracy: 0.7203 - val_loss: 3.8738 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 13s 653ms/step - loss: 0.9692 - accuracy: 0.7203 - val_loss: 3.9202 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 12s 603ms/step - loss: 1.0282 - accuracy: 0.7203 - val_loss: 4.5801 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 13s 613ms/step - loss: 0.9982 - accuracy: 0.7203 - val_loss: 4.4886 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 12s 584ms/step - loss: 1.0046 - accuracy: 0.7203 - val_loss: 4.9340 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a0494\\PycharmProjects\\interp\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "# Load the CSV file with image names and classes\n",
    "csv_path = r\"C:\\Users\\a0494\\PycharmProjects\\int20h\\emotion_labels.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df['Label'] = df['Label'].astype(str)\n",
    "# Define image dimensions and classes\n",
    "img_width, img_height, img_channels = 128, 128, 3\n",
    "num_classes = len(df['Label'].unique())\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "# Data augmentation using ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # split the data into training and validation sets\n",
    ")\n",
    "\n",
    "# Create generators for training and validation data\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=r\"D:\\Andrii\\styding\\hachathons\\Int20h\\crop_sorted_images\",\n",
    "    x_col='Filename',\n",
    "    y_col='Label',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "valid_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=r\"D:\\Andrii\\styding\\hachathons\\Int20h\\crop_sorted_images\",\n",
    "    x_col='Filename',\n",
    "    y_col='Label',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# Freeze the pre-trained layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model on top\n",
    "model = models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10  # You can adjust this based on your dataset and computational resources\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save('fine_tuned_resnet50.h5')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}